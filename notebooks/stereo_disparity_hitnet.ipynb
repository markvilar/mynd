{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3413621b-2dfc-4485-9d30-20f34fbcb460",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.cache/pypoetry/virtualenvs/benthoscan-DFS0007P-py3.11/lib/python3.11/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import Metashape\n",
    "\n",
    "from result import Ok, Err, Result\n",
    "\n",
    "from benthoscan.backends import metashape as backend\n",
    "from benthoscan.utils.log import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da21757e-2d86-41db-bcf6-12b47b0361d1",
   "metadata": {},
   "source": [
    "### Define workflow for obtaining rectification results and image pairs from Metashape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d2f781-4352-49f8-979b-492e15e85968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tqdm\n",
    "\n",
    "from benthoscan.data.image import Image, ImagePair\n",
    "\n",
    "from benthoscan.backends.metashape.data_types import (\n",
    "    SensorPair, \n",
    "    CameraPair,\n",
    "    StereoGroup,\n",
    ")\n",
    "\n",
    "from benthoscan.backends.metashape.camera_helpers import (\n",
    "    compute_stereo_calibration,\n",
    "    get_stereo_groups,\n",
    ")\n",
    "\n",
    "from benthoscan.backends.metashape.image_helpers import (\n",
    "    ImagePairLoader,\n",
    "    generate_image_loaders,\n",
    ")\n",
    "\n",
    "from benthoscan.geometry.stereo import (\n",
    "    StereoCalibration,\n",
    "    RectificationResult,\n",
    "    compute_stereo_rectification,\n",
    "    rectify_image_pair,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_stereo_rectification(chunk: Metashape.Chunk) -> tuple[RectificationResult, list[ImagePairLoader]]:\n",
    "    \"\"\"Returns rectification results and image loaders for the stereo groups in a Metashape chunk.\"\"\"\n",
    "\n",
    "    # Get pairs of sensors and cameras\n",
    "    stereo_groups: list[StereoGroup] = get_stereo_groups(chunk)\n",
    "\n",
    "    # NOTE: Process the first stereo group for now\n",
    "    group: StereoGroup = stereo_groups[0]\n",
    "\n",
    "    # Compute the stereo calibration for the sensor pair\n",
    "    calibration: StereoCalibration = compute_stereo_calibration(group.sensor_pair)\n",
    "\n",
    "    # Compute homographies, image transformations and camera matrices for stereo rectification\n",
    "    rectification: RectificationResult = compute_stereo_rectification(calibration)\n",
    "    \n",
    "    # Set up image loaders for all camera pairs\n",
    "    image_loaders: list[ImagePairLoader] = generate_image_loaders(group.camera_pairs)\n",
    "\n",
    "    return rectification, image_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d250c-a004-4c8b-9deb-80cf53e76087",
   "metadata": {},
   "source": [
    "### Define HITNET functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af6bf71-dce5-4a40-a4ee-0918a1d71007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import NamedTuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as onnxrt\n",
    "\n",
    "from benthoscan.data.image import Image, ImageFormat\n",
    "\n",
    "MODEL_REPOSITORY: str = \"https://github.com/nburrus/stereodemo/releases/download/v0.1-hitnet\"\n",
    "\n",
    "URLS: dict[str, str] = {\n",
    "    \"hitnet_eth3d_120x160.onnx\": f\"{MODEL_REPOSITORY}/hitnet_eth3d_120x160.onnx\",\n",
    "    \"hitnet_eth3d_240x320.onnx\": f\"{MODEL_REPOSITORY}/hitnet_eth3d_240x320.onnx\",\n",
    "    \"hitnet_eth3d_480x640.onnx\": f\"{MODEL_REPOSITORY}/hitnet_eth3d_480x640.onnx\",\n",
    "    \"hitnet_eth3d_720x1280.onnx\": f\"{MODEL_REPOSITORY}/hitnet_eth3d_720x1280.onnx\",\n",
    "    \"hitnet_middlebury_120x160.onnx\": f\"{MODEL_REPOSITORY}/hitnet_middlebury_120x160.onnx\",\n",
    "    \"hitnet_middlebury_240x320.onnx\": f\"{MODEL_REPOSITORY}/hitnet_middlebury_240x320.onnx\",\n",
    "    \"hitnet_middlebury_480x640.onnx\": f\"{MODEL_REPOSITORY}/hitnet_middlebury_480x640.onnx\",\n",
    "    \"hitnet_middlebury_720x1280.onnx\": f\"{MODEL_REPOSITORY}/hitnet_middlebury_720x1280.onnx\",\n",
    "    \"hitnet_sceneflow_120x160.onnx\": f\"{MODEL_REPOSITORY}/hitnet_sceneflow_120x160.onnx\",\n",
    "    \"hitnet_sceneflow_240x320.onnx\": f\"{MODEL_REPOSITORY}/hitnet_sceneflow_240x320.onnx\",\n",
    "    \"hitnet_sceneflow_480x640.onnx\": f\"{MODEL_REPOSITORY}/hitnet_sceneflow_480x640.onnx\",\n",
    "    \"hitnet_sceneflow_720x1280.onnx\": f\"{MODEL_REPOSITORY}/hitnet_sceneflow_720x1280.onnx\",\n",
    "}\n",
    "\n",
    "\n",
    "class Argument(NamedTuple):\n",
    "    \"\"\"Class representing an argument.\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    shape: tuple\n",
    "    type: type\n",
    "    \n",
    "\n",
    "class HitnetConfig(NamedTuple):\n",
    "    \"\"\"Class representing a Hitnet config.\"\"\"\n",
    "\n",
    "    session: onnxrt.InferenceSession\n",
    "\n",
    "    @property\n",
    "    def inputs(self) -> list[Argument]:\n",
    "        \"\"\"Returns the inputs of the session.\"\"\"\n",
    "        arguments = self.session.get_inputs()\n",
    "        return [Argument(argument.name, tuple(argument.shape), argument.type) for argument in arguments]\n",
    "\n",
    "    @property\n",
    "    def outputs(self) -> list[Argument]:\n",
    "        \"\"\"Returns the inputs of the session.\"\"\"\n",
    "        arguments = self.session.get_outputs()\n",
    "        return [Argument(argument.name, tuple(argument.shape), argument.type) for argument in arguments]\n",
    "\n",
    "    @property\n",
    "    def input_size(self) -> tuple[int, int]:\n",
    "        \"\"\"Returns the expected input size for the model as (H, W).\"\"\"\n",
    "        tensor_argument: Argument = self.inputs[0]\n",
    "        batch, channels, height, width = tensor_argument.shape\n",
    "        return (height, width)\n",
    "\n",
    "\n",
    "def load_hitnet(path: Path) -> HitnetConfig:\n",
    "    \"\"\"Loads a Hitnet model from an ONNX file.\"\"\"\n",
    "\n",
    "    if not path.exists():\n",
    "        return Err(f\"model path does not exist: {path}\")\n",
    "    if not path.suffix == \".onnx\":\n",
    "        return Err(f\"model path is not an ONNX file: {path}\")\n",
    "    \n",
    "    session: onnxrt.InferenceSession = onnxrt.InferenceSession(\n",
    "        str(path), \n",
    "        providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "    )\n",
    "\n",
    "    return HitnetConfig(session = session)\n",
    "\n",
    "\n",
    "def _preprocess_images(config: HitnetConfig, left: Image, right: Image) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Preprocess input images for HITNET.\"\"\"\n",
    "\n",
    "    match left.format:\n",
    "        case ImageFormat.RGB:\n",
    "            left_array: np.ndarray = cv2.cvtColor(left.to_array(), cv2.COLOR_RGB2GRAY)\n",
    "        case ImageFormat.BGR:\n",
    "            left_array: np.ndarray = cv2.cvtColor(left.to_array(), cv2.COLOR_BGR2GRAY)\n",
    "        case ImageFormat.GRAY:\n",
    "            left_array: np.ndarray = left.to_array()\n",
    "        case _:\n",
    "            raise NotImplementedError(f\"invalid image format: {left.format}\")\n",
    "\n",
    "    match right.format:\n",
    "        case ImageFormat.RGB:\n",
    "            right_array: np.ndarray = cv2.cvtColor(right.to_array(), cv2.COLOR_RGB2GRAY)\n",
    "        case ImageFormat.BGR:\n",
    "            right_array: np.ndarray = cv2.cvtColor(right.to_array(), cv2.COLOR_BGR2GRAY)\n",
    "        case ImageFormat.GRAY:\n",
    "            right_array: np.ndarray = right.to_array()\n",
    "        case _:\n",
    "            raise NotImplementedError(f\"invalid image format: {right.format}\")\n",
    "\n",
    "    # NOTE: Images should now be grayscale\n",
    "\n",
    "    assert len(config.inputs) == 1, f\"invalid number of inputs: {len(config.inputs)}\"\n",
    "    assert len(config.outputs) == 1, f\"invalid number of outputs: {len(config.outputs)}\"\n",
    "\n",
    "    height, width = config.input_size\n",
    "\n",
    "    left_array: np.ndarray = cv2.resize(left_array, (width, height), cv2.INTER_AREA)\n",
    "    right_array: np.ndarray = cv2.resize(right_array, (width, height), cv2.INTER_AREA)\n",
    "    \n",
    "    # Grayscale needs expansion to reach H,W,C.\n",
    "    # Need to do that now because resize would change the shape.\n",
    "    if left_array.ndim == 2:\n",
    "        left_array: np.ndarray = np.expand_dims(left_array, axis=-1)\n",
    "    if right_array.ndim == 2:\n",
    "        right_array: np.ndarray = np.expand_dims(right_array, axis=-1)\n",
    "\n",
    "    # TODO: Get normalization value based on image dtype\n",
    "\n",
    "    # -> H,W,C=2 or 6 , normalized to [0,1]\n",
    "    tensor = np.concatenate((left_array, right_array), axis=-1) / 255.0\n",
    "    # -> C,H,W\n",
    "    tensor = tensor.transpose(2, 0, 1)\n",
    "    # -> B=1,C,H,W\n",
    "    tensor = np.expand_dims(tensor, 0).astype(np.float32)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "def _postprocess_disparity(disparity: np.ndarray, image: Image, flip: bool=False) -> np.ndarray:\n",
    "    \"\"\"Postprocess the disparity map by resizing it to match the original image, \n",
    "    adjusting the disparity with the width ratio, and optionally flipping the disparity \n",
    "    horizontally.\"\"\"\n",
    "\n",
    "    # Squeeze disparity to a 2D array\n",
    "    disparity: np.ndarray = np.squeeze(disparity)\n",
    "\n",
    "    # Scale disparities by the width ratios between the original images and the disparity maps\n",
    "    scale: float = float(image.width) / float(disparity.shape[1])\n",
    "    disparity *= scale\n",
    "\n",
    "    # Resize disparity maps to the original image sizes\n",
    "    disparity: np.ndarray = cv2.resize(disparity, (image.width, image.height), cv2.INTER_AREA)\n",
    "\n",
    "    # If enabled, flip disparity map around y-axis (horizontally)\n",
    "    if flip:\n",
    "        disparity: np.ndarray = cv2.flip(disparity, 1)\n",
    "        \n",
    "    return disparity\n",
    "\n",
    "\n",
    "def compute_disparity(config: HitnetConfig, left: Image, right: Image) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Computes the disparity for a pair of stereo images. The images needs to be \n",
    "    rectified prior to disparity estimation. Returns the left and right disparity as\n",
    "    arrays with float32 values.\"\"\"\n",
    "\n",
    "    # Create tensor from flipped images to get left disparity\n",
    "    flipped_left: Image = Image(\n",
    "        data=cv2.flip(left.to_array(), 1), \n",
    "        format=left.format,\n",
    "    )\n",
    "    flipped_right: Image = Image(\n",
    "        data=cv2.flip(right.to_array(), 1), \n",
    "        format=right.format,\n",
    "    )\n",
    "\n",
    "    tensor: np.ndarray = _preprocess_images(config, left, right)\n",
    "    flipped_tensor: np.ndarray = _preprocess_images(config, flipped_right, flipped_left)\n",
    "\n",
    "    left_outputs: list[np.ndarray] = config.session.run([\"reference_output_disparity\"], { \"input\": tensor })\n",
    "    right_outputs: list[np.ndarray] = config.session.run([\"reference_output_disparity\"], { \"input\": flipped_tensor })\n",
    "\n",
    "    # Since we estimate the right disparity from the flipped images, we need to flip the\n",
    "    # right disparity map back to the same perspective as the original rigth image\n",
    "    left_disparity: np.ndarray = _postprocess_disparity(left_outputs[0], left, flip=False)\n",
    "    right_disparity: np.ndarray = _postprocess_disparity(right_outputs[0], right, flip=True)\n",
    "\n",
    "    return left_disparity, right_disparity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888c45e-03e1-438b-94e1-fabde4ea3098",
   "metadata": {},
   "source": [
    "### Load a ONNX model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f3f3d19-3e83-4147-992f-30290758e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import tqdm\n",
    "\n",
    "from benthoscan.io import write_image\n",
    "from benthoscan.runtime import Environment, load_environment\n",
    "from benthoscan.geometry.range_maps import compute_range_from_disparity, compute_normals_from_range\n",
    "\n",
    "\n",
    "def estimate_stereo_geometry_and_export(\n",
    "    rectification: RectificationResult, \n",
    "    image_loaders: list[ImagePairLoader],\n",
    "    model: HitnetConfig,\n",
    "    directories: dict[str, Path],\n",
    ") -> None:\n",
    "    \"\"\"Function for developing Hitnet functionality.\"\"\"\n",
    "\n",
    "    focal_length: float = rectification.master.calibration.focal_length\n",
    "    baseline: float = rectification.slave.location[0]\n",
    "\n",
    "    for loader in tqdm.tqdm(image_loaders, desc=\"Estimating disparity...\"):\n",
    "        images: ImagePair = loader()\n",
    "\n",
    "        rectified_images: tuple = rectify_image_pair(\n",
    "            master_image=images.first.to_array(),\n",
    "            slave_image=images.second.to_array(),\n",
    "            rectification=rectification,\n",
    "        )\n",
    "        \n",
    "        left_rect: Image = Image(data=rectified_images[0], format=images.first.format)\n",
    "        right_rect: Image = Image(data=rectified_images[1], format=images.second.format)\n",
    "        \n",
    "        disparity_maps: tuple[np.ndarray, np.ndarray] = compute_disparity(\n",
    "            model, \n",
    "            left=left_rect, \n",
    "            right=right_rect\n",
    "        )\n",
    "\n",
    "        left_ranges: np.ndarray = compute_range_from_disparity(\n",
    "            disparity=disparity_maps[0], \n",
    "            baseline=baseline, \n",
    "            focal_length=focal_length,\n",
    "        )\n",
    "\n",
    "        right_ranges: np.ndarray = compute_range_from_disparity(\n",
    "            disparity=disparity_maps[1], \n",
    "            baseline=baseline,\n",
    "            focal_length=focal_length,\n",
    "        )\n",
    "\n",
    "        left_normals: np.ndarray = compute_normals_from_range(\n",
    "            range_map = left_ranges, \n",
    "            camera_matrix = rectification.master.calibration.camera_matrix, \n",
    "            flipped = True,\n",
    "        )\n",
    "\n",
    "        right_normals: np.ndarray = compute_normals_from_range(\n",
    "            range_map = right_ranges, \n",
    "            camera_matrix = rectification.slave.calibration.camera_matrix, \n",
    "            flipped = True,\n",
    "        )\n",
    "\n",
    "        # Convert to 16 bit to save storage space\n",
    "        left_ranges: np.ndarray = left_ranges.astype(np.float16)\n",
    "        right_ranges: np.ndarray = right_ranges.astype(np.float16)\n",
    "        left_normals: np.ndarray = left_normals.astype(np.float16)\n",
    "        right_normals: np.ndarray = right_normals.astype(np.float16)\n",
    "\n",
    "        paths: dict[str, Path] = {\n",
    "            \"left_ranges\": directories.get(\"ranges\") / f\"{images.first.label}.tiff\",\n",
    "            \"right_ranges\": directories.get(\"ranges\") / f\"{images.second.label}.tiff\",\n",
    "            \"left_normals\": directories.get(\"normals\") / f\"{images.first.label}.tiff\",\n",
    "            \"right_normals\": directories.get(\"normals\") / f\"{images.second.label}.tiff\",\n",
    "        }\n",
    "\n",
    "        write_image(uri=paths.get(\"left_ranges\"), image=left_ranges)\n",
    "        write_image(uri=paths.get(\"right_ranges\"), image=right_ranges)\n",
    "        write_image(uri=paths.get(\"left_normals\"), image=left_normals)\n",
    "        write_image(uri=paths.get(\"right_normals\"), image=right_normals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0755b9b-6c17-4266-961c-e7745e3a536c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-22 16:23:59.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mloaded environment\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoadProject: path = /data/kingston_snv_01/acfr_revisits_metashape_projects/r23685bc_working_version.psz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-22 16:24:26.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mloaded document /data/kingston_snv_01/acfr_revisits_metashape_projects/r23685bc_working_version.psz successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded project in 27.1366 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating disparity...:   4%|███████▌                                                                                                                                                                                           | 106/2728 [04:58<2:03:42,  2.83s/it]"
     ]
    }
   ],
   "source": [
    "PATHS: dict[str, Path] = {\n",
    "    \"DOCUMENT_IN\": Path(\n",
    "        \"/data/kingston_snv_01/acfr_revisits_metashape_projects/r23685bc_working_version.psz\"\n",
    "    ),\n",
    "    \"DOCUMENT_OUT\": Path(\n",
    "        \"/data/kingston_snv_01/acfr_revisits_metashape_projects_test/r23685bc_working_version_saved.psz\"\n",
    "    ),\n",
    "    \"CACHE\": Path(\"/home/martin/dev/benthoscan/.cache/\"),\n",
    "}\n",
    "\n",
    "\n",
    "result: Result[Environment, str] = load_environment()\n",
    "\n",
    "match result:\n",
    "    case Ok(environment):\n",
    "        logger.info(\"loaded environment\")\n",
    "    case Err(message):\n",
    "        logger.error(message)\n",
    "\n",
    "result: Result[str, str] = backend.load_project(PATHS.get(\"DOCUMENT_IN\"))\n",
    "match result:\n",
    "    case Ok(message):\n",
    "        logger.info(message)\n",
    "    case Err(message):\n",
    "        logger.error(message)\n",
    "\n",
    "document: Metashape.Document = backend.context._backend_data.get(\"document\")\n",
    "\n",
    "target_labels: list[str] = [ \"r23685bc_20100605_021022\" ]\n",
    "target_chunks: list[Metashape.Chunk] = [chunk for chunk in document.chunks if chunk.label in target_labels]\n",
    "\n",
    "directories: dict[str, Path] = {\n",
    "    \"ranges\": Path(\"/data/kingston_snv_01/stereo_range_maps\"),\n",
    "    \"normals\": Path(\"/data/kingston_snv_01/stereo_normal_maps\"),\n",
    "}\n",
    "\n",
    "# Load model\n",
    "model: HitnetConfig = load_hitnet(environment.resource_directory / Path(\"hitnet_models/hitnet_eth3d_720x1280.onnx\"))\n",
    "\n",
    "# Generate range maps based on stereo pairs\n",
    "for chunk in target_chunks:\n",
    "\n",
    "    output_directories = {key: path / f\"{chunk.label}\" for key, path in directories.items()}\n",
    "    \n",
    "    rectification, image_loaders = generate_stereo_rectification(chunk)\n",
    "\n",
    "    # NOTE: Test Hitnet workflow - Development purposes only!\n",
    "    estimate_stereo_geometry_and_export(rectification, image_loaders, model, output_directories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
